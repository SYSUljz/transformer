{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb50d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 处理完成: 2018-09-25.csv, 区间长度=112\n",
      "✅ 处理完成: 2018-05-31.csv, 区间长度=215\n",
      "✅ 处理完成: 2017-03-11.csv, 区间长度=95\n",
      "✅ 处理完成: 2017-06-23.csv, 区间长度=105\n",
      "✅ 处理完成: 2015-05-30.csv, 区间长度=131\n",
      "✅ 处理完成: 2016-05-10.csv, 区间长度=160\n",
      "✅ 处理完成: 2017-06-26.csv, 区间长度=82\n",
      "✅ 处理完成: 2015-12-10.csv, 区间长度=126\n",
      "✅ 处理完成: 2015-05-21.csv, 区间长度=76\n",
      "✅ 处理完成: 2016-07-10.csv, 区间长度=67\n",
      "✅ 处理完成: 2019-01-19.csv, 区间长度=235\n",
      "✅ 处理完成: 2018-07-19.csv, 区间长度=272\n",
      "✅ 处理完成: 2018-09-08.csv, 区间长度=221\n",
      "✅ 处理完成: 2018-05-08.csv, 区间长度=74\n",
      "✅ 处理完成: 2015-12-06.csv, 区间长度=102\n",
      "✅ 处理完成: 2018-10-28.csv, 区间长度=80\n",
      "✅ 处理完成: 2016-10-23.csv, 区间长度=79\n",
      "✅ 处理完成: 2019-06-13.csv, 区间长度=81\n",
      "✅ 处理完成: 2018-07-30.csv, 区间长度=60\n",
      "✅ 处理完成: 2018-09-21.csv, 区间长度=108\n",
      "✅ 处理完成: 2016-03-10.csv, 区间长度=118\n",
      "✅ 处理完成: 2014-05-26.csv, 区间长度=134\n",
      "✅ 处理完成: 2017-07-03.csv, 区间长度=125\n",
      "✅ 处理完成: 2016-01-29.csv, 区间长度=100\n",
      "✅ 处理完成: 2018-08-13.csv, 区间长度=66\n",
      "✅ 处理完成: 2019-02-05.csv, 区间长度=78\n",
      "✅ 处理完成: 2016-04-18.csv, 区间长度=172\n",
      "✅ 处理完成: 2018-06-15.csv, 区间长度=141\n",
      "✅ 处理完成: 2018-08-04.csv, 区间长度=55\n",
      "✅ 处理完成: 2016-04-23.csv, 区间长度=102\n",
      "✅ 处理完成: 2018-11-21.csv, 区间长度=98\n",
      "✅ 处理完成: 2016-06-27.csv, 区间长度=90\n",
      "✅ 处理完成: 2018-06-10.csv, 区间长度=64\n",
      "✅ 处理完成: 2018-03-20.csv, 区间长度=50\n",
      "✅ 处理完成: 2014-05-23.csv, 区间长度=87\n",
      "✅ 处理完成: 2018-08-17.csv, 区间长度=125\n",
      "✅ 处理完成: 2015-06-05.csv, 区间长度=101\n",
      "✅ 处理完成: 2015-06-11.csv, 区间长度=86\n",
      "✅ 处理完成: 2018-06-07.csv, 区间长度=116\n",
      "✅ 处理完成: 2019-05-21.csv, 区间长度=60\n",
      "✅ 处理完成: 2016-04-21.csv, 区间长度=68\n",
      "✅ 处理完成: 2016-03-14.csv, 区间长度=112\n",
      "✅ 处理完成: 2016-04-09.csv, 区间长度=153\n",
      "✅ 处理完成: 2013-08-23.csv, 区间长度=49\n",
      "✅ 处理完成: 2018-08-27.csv, 区间长度=76\n",
      "✅ 处理完成: 2018-06-23.csv, 区间长度=41\n",
      "✅ 处理完成: 2014-05-12.csv, 区间长度=93\n",
      "✅ 处理完成: 2016-03-24.csv, 区间长度=136\n",
      "✅ 处理完成: 2017-05-26.csv, 区间长度=230\n",
      "✅ 处理完成: 2016-06-16.csv, 区间长度=183\n",
      "✅ 处理完成: 2015-11-13.csv, 区间长度=139\n",
      "✅ 处理完成: 2016-11-27.csv, 区间长度=161\n",
      "✅ 处理完成: 2018-06-20.csv, 区间长度=60\n",
      "✅ 处理完成: 2019-07-03.csv, 区间长度=39\n",
      "✅ 处理完成: 2018-08-25.csv, 区间长度=45\n",
      "✅ 处理完成: 2018-03-04.csv, 区间长度=45\n",
      "✅ 处理完成: 2014-05-15.csv, 区间长度=103\n",
      "✅ 处理完成: 2015-11-17.csv, 区间长度=155\n",
      "✅ 处理完成: 2019-05-17.csv, 区间长度=49\n",
      "✅ 处理完成: 2016-11-20.csv, 区间长度=117\n",
      "✅ 处理完成: 2018-11-03.csv, 区间长度=54\n",
      "✅ 处理完成: 2018-11-16.csv, 区间长度=64\n",
      "✅ 处理完成: 2018-03-02.csv, 区间长度=106\n",
      "✅ 处理完成: 2019-02-09.csv, 区间长度=23\n",
      "✅ 处理完成: 2016-03-21.csv, 区间长度=71\n",
      "✅ 处理完成: 2014-06-21.csv, 区间长度=89\n",
      "✅ 处理完成: 2017-06-29.csv, 区间长度=130\n",
      "✅ 处理完成: 2015-12-23.csv, 区间长度=139\n",
      "✅ 处理完成: 2017-03-31.csv, 区间长度=155\n",
      "✅ 处理完成: 2018-02-25.csv, 区间长度=103\n",
      "✅ 处理完成: 2017-08-11.csv, 区间长度=126\n",
      "✅ 处理完成: 2018-10-18.csv, 区间长度=162\n",
      "✅ 处理完成: 2018-09-11.csv, 区间长度=100\n",
      "✅ 处理完成: 2018-07-14.csv, 区间长度=184\n",
      "✅ 处理完成: 2019-06-21.csv, 区间长度=190\n",
      "✅ 处理完成: 2017-06-02.csv, 区间长度=136\n",
      "✅ 处理完成: 2018-02-27.csv, 区间长度=67\n",
      "✅ 处理完成: 2019-04-25.csv, 区间长度=70\n",
      "✅ 处理完成: 2016-02-10.csv, 区间长度=87\n",
      "✅ 处理完成: 2017-06-17.csv, 区间长度=128\n",
      "✅ 处理完成: 2014-06-23.csv, 区间长度=128\n",
      "✅ 处理完成: 2019-06-08.csv, 区间长度=59\n",
      "✅ 处理完成: 2018-09-16.csv, 区间长度=145\n",
      "✅ 处理完成: 2015-07-04.csv, 区间长度=211\n",
      "✅ 处理完成: 2016-07-18.csv, 区间长度=174\n",
      "✅ 处理完成: 2016-05-21.csv, 区间长度=109\n",
      "✅ 处理完成: 2015-05-28.csv, 区间长度=74\n",
      "✅ 处理完成: 2016-05-08.csv, 区间长度=39\n",
      "✅ 处理完成: 2019-06-25.csv, 区间长度=163\n",
      "✅ 处理完成: 2018-10-20.csv, 区间长度=105\n",
      "✅ 处理完成: 2018-07-05.csv, 区间长度=208\n",
      "\n",
      "✅ 已保存合并文件: /Users/jackli/Desktop/python/transformer/processed_data/merged_max_nonmissing.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bv/k15hzqs91t9crml70zw5t5qr0000gn/T/ipykernel_28689/2929487358.py:66: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(valid_dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# 合并文件\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 文件夹路径\n",
    "folder_path = \"/Users/jackli/Desktop/python/transformer/processed_data\"\n",
    "\n",
    "# 获取该文件夹下所有 csv 文件路径\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "all_columns = set()  # 收集所有列名\n",
    "valid_dfs = []       # 存放处理后的DataFrame\n",
    "\n",
    "# 第一次遍历：收集所有列名\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        all_columns.update(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取文件失败 {file}: {e}\")\n",
    "\n",
    "# 转成固定顺序的列表\n",
    "all_columns = list(all_columns)\n",
    "\n",
    "# 第二次遍历：提取最大连续非缺失区间\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        column = 'flow'\n",
    "\n",
    "        # 跳过不含flow列或该列全为NaN的文件\n",
    "        if column not in df.columns or df[column].dropna().empty:\n",
    "            print(f\"⚠️ 跳过文件（无有效flow数据）: {os.path.basename(file)}\")\n",
    "            continue\n",
    "\n",
    "        # 标识连续非缺失区间\n",
    "        mask = df[column].notna()\n",
    "        df['group'] = (mask != mask.shift()).cumsum()\n",
    "\n",
    "        # 找出最大非缺失区间\n",
    "        non_missing_groups = df[mask].groupby('group').size()\n",
    "        max_group = non_missing_groups.idxmax()\n",
    "\n",
    "        # 保留最大连续区间\n",
    "        df_cleaned = df[df['group'] == max_group].drop(columns='group').reset_index(drop=True)\n",
    "\n",
    "        # 补齐缺失列\n",
    "        for col in all_columns:\n",
    "            if col not in df_cleaned.columns:\n",
    "                df_cleaned[col] = pd.NA\n",
    "\n",
    "        # 新增来源文件列\n",
    "        df_cleaned['source_file'] = os.path.basename(file).split(\".\")[0]\n",
    "\n",
    "        # 保持列顺序（source_file放最后）\n",
    "        df_cleaned = df_cleaned[all_columns + ['source_file']]\n",
    "\n",
    "        valid_dfs.append(df_cleaned)\n",
    "        print(f\"✅ 处理完成: {os.path.basename(file)}, 区间长度={len(df_cleaned)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理文件出错 {file}: {e}\")\n",
    "\n",
    "# 合并所有处理后的文件\n",
    "if valid_dfs:\n",
    "    merged_df = pd.concat(valid_dfs, ignore_index=True)\n",
    "    output_path = os.path.join(folder_path, \"merged_max_nonmissing.csv\")\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✅ 已保存合并文件: {output_path}\")\n",
    "else:\n",
    "    print(\"⚠️ 未找到有效文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb5948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已保存标准化文件: /Users/jackli/Desktop/python/transformer/processed_data/merged_max_nonmissing_scaled.csv\n",
      "Flow 均值=5.8329, 标准差=0.6035\n"
     ]
    }
   ],
   "source": [
    "#标准化\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 文件路径\n",
    "folder_path = \"/Users/jackli/Desktop/python/transformer/processed_data\"\n",
    "input_path = os.path.join(folder_path, \"merged_max_nonmissing.csv\")\n",
    "output_path = os.path.join(folder_path, \"merged_max_nonmissing_scaled.csv\")\n",
    "\n",
    "# 读取文件\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 检查列\n",
    "if \"flow\" not in df.columns:\n",
    "    raise ValueError(\"未在文件中找到 'flow' 列，请确认输入文件是否正确。\")\n",
    "\n",
    "# 复制一份避免修改原数据\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# 对除 source_file 以外的所有列尝试做 log1p\n",
    "numeric_cols = [col for col in df.columns if col != \"source_file\"]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    # 非数值列会被跳过\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        df_scaled[col] = np.log1p(df[col])\n",
    "    else:\n",
    "        print(f\"⚠️ 跳过非数值列: {col}\")\n",
    "\n",
    "# 对 flow 列进一步标准化\n",
    "flow_log = df_scaled[\"flow\"].copy()\n",
    "\n",
    "# 只保留非零值（用于计算均值和方差）\n",
    "flow_log_nonzero = flow_log[flow_log > 0]\n",
    "\n",
    "mean = flow_log_nonzero.mean()\n",
    "std = flow_log_nonzero.std()\n",
    "\n",
    "# 对非零部分标准化\n",
    "flow_scaled = (flow_log - mean) / std\n",
    "df_scaled[\"flow\"] = flow_scaled\n",
    "\n",
    "# 输出结果\n",
    "df_scaled.to_csv(output_path, index=False)\n",
    "print(f\"✅ 已保存标准化文件: {output_path}\")\n",
    "print(f\"Flow 均值={mean:.4f}, 标准差={std:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptotradelib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
